{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49f08e76-269f-4aef-9ee3-ed8cf0565af1",
   "metadata": {},
   "source": [
    "# 2. Building the vector database\n",
    "- we load our previously created collections from the database\n",
    "- we use a pipeline to get the raw profile data as well as a unique identifier for each profile\n",
    "- we split the profiles into chunks\n",
    "- we embed the chunks\n",
    "- we store the embeddings in a vector database\n",
    "- we can now query our vector database for semantic similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e00c546-1be9-4e3c-a64c-9c29116bf2a7",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8095fea4-8598-47f4-9b20-29bac8819f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cf297a7-a348-4066-885d-110963670dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the client\n",
    "client = MongoClient()\n",
    "\n",
    "# The database on the client we're connecting to\n",
    "db = client['zhaw_matchmaking']\n",
    "\n",
    "# The collections in the database we are using\n",
    "profile_data_collection = db['profile_data']\n",
    "persons_collection = db[\"persons\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3b0919-899f-44fd-8e18-3c65a94b883a",
   "metadata": {},
   "source": [
    "### Use a pipeline to retrieve the profile data and the unique `shorthandSymbol` for each user\n",
    "- we access both collections\n",
    "- we use a generator for memory efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "634a93e2-5db7-4bec-8829-d78b6f972af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our aggregation pipeline\n",
    "pipeline = [\n",
    "    {\n",
    "        \"$lookup\": {\n",
    "            \"from\": \"persons\",\n",
    "            \"localField\": \"person_id\",\n",
    "            \"foreignField\": \"_id\",\n",
    "            \"as\": \"person_data\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"$unwind\": \"$person_data\"\n",
    "    },\n",
    "    {\n",
    "        \"$project\": {\n",
    "            \"raw_data\": 1,\n",
    "            \"shorthandSymbol\": \"$person_data.shorthandSymbol\"\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89171a42-3fa1-4737-b197-f5ab74ae93cf",
   "metadata": {},
   "source": [
    "#### Define a generator function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6bc2f07-3481-43f0-891d-26db671cae43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_profiles():\n",
    "    for profile in profile_data_collection.aggregate(pipeline):\n",
    "        yield profile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587c754f-af82-406d-bdda-edbfbf721a0e",
   "metadata": {},
   "source": [
    "## Preprocess the profile data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3855df9-2ba6-4f63-a095-f787248a5d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b73569ff-1336-4f1c-91b6-7e7aa576ed7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_profile(raw_data: str):\n",
    "    # Remove all obsolete whitespace\n",
    "    cleaned_str = re.sub(r'\\s+', ' ', raw_data)\n",
    "\n",
    "    # Remove the final \"Zurück\" in every profile\n",
    "    cleaned_str = cleaned_str.replace('Zurück', '')\n",
    "\n",
    "    # Insert whitespace between numbers and letters (when they got lost from the original html)\n",
    "    pattern = r'([a-zA-Z])(\\d)'\n",
    "    cleaned_str = re.sub(pattern, r'\\1 \\2', cleaned_str)\n",
    "\n",
    "    return cleaned_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9960a8da-e37e-41ad-ab0b-15bd97f5e6bc",
   "metadata": {},
   "source": [
    "## Load the profiles as documents for LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "021c5327-fcbd-47ff-9f06-c6d30d6373a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.base import BaseLoader\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f39ae26-addf-4748-8d8c-0cc298c85e88",
   "metadata": {},
   "source": [
    "### Define a custom loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c8902f0-9298-4997-bf36-bc9381a06e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoader(BaseLoader):\n",
    "    def __init__(self, generate_profiles):\n",
    "        self.profiles = generate_profiles\n",
    "\n",
    "    def lazy_load(self):\n",
    "        for profile in self.profiles:\n",
    "            metadata = {\"source\": profile[\"shorthandSymbol\"]}\n",
    "            page_content = preprocess_profile(profile[\"raw_data\"])\n",
    "            yield Document(page_content=page_content, metadata=metadata)\n",
    "\n",
    "    def load(self):\n",
    "        return list(self.lazy_load())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fda1b1e-f755-4972-8166-4544dbce29a5",
   "metadata": {},
   "source": [
    "### Instantiate the custom loader and load the profiles as documents\n",
    "- we use the custom loader to load our profile data\n",
    "- each profile will be a `Document`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "315185f5-37c9-4ec5-98b4-dfd71e270ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = CustomLoader(generate_profiles())\n",
    "loaded_documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70b1c08-9acc-4165-a27b-2981dfc8b935",
   "metadata": {},
   "source": [
    "## Chunking\n",
    "- as our profile data might be too large as context for a prompt, we need to split each profile (loaded document) into smaller chunks\n",
    "- no overlap necessary\n",
    "- `RecursiveCharacterTextSplitter` is recommended for generic text\n",
    "- each loaded document will be split into chunks, these chunks will be held in `list_of_document_chunks`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c16232b-91f0-4248-8a2e-8a2f1928ad20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d162eef6-ad60-4191-bf15-1659f995edcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate text splitter with chunk size of 500 characters\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52562a0a-f8ea-4605-942d-f43bb0ebbf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_document_chunks = text_splitter.split_documents(loaded_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d530b3a-0520-4cd2-8120-752f8529f611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57834\n"
     ]
    }
   ],
   "source": [
    "print(len(list_of_document_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4322bc29-a7fa-4079-96fd-b3af905651aa",
   "metadata": {},
   "source": [
    "## Create embeddings from the generated chunks and add to our Vector database\n",
    "- **make sure to place your OpenAI API key in `.env`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdebfa1c-fdd9-4014-b612-e6e6271f2843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35c9bd22-101f-4ece-b659-d314df51e284",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "embedding_function = OpenAIEmbeddings(show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df4dc70-9587-4b50-a682-d208c6104940",
   "metadata": {},
   "source": [
    "### Create a `ChromaDB` from our generated list of document chunks and save it to disk\n",
    "- this will take a while, the final database is around `1GB`\n",
    "- to avoid getting a `RateLimitError`, we use the `tenacity` library to retry the call after an exponential delay\n",
    "- we also call a custom function to fill the database with splits of the generated chunks (Chroma can handle max. 41000 at once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "069af853-ac00-4d06-a062-03cdb3936f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")  # for exponential backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b5bbf9d-1b48-44c8-86c8-6d7e0ec27fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=20, max=120), stop=stop_after_attempt(20))\n",
    "def create_vectordb(split_chunks):\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=split_chunks,\n",
    "        embedding=embedding_function,\n",
    "        persist_directory=\"./chroma_db\")\n",
    "    vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f7580ef-c613-4c2b-a0a8-e1c64fed2697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list_of_document_chunks(input_list, split_size):\n",
    "    for i in range(0, len(input_list), split_size):\n",
    "        yield input_list[i:i + split_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "65045a71-0fe2-4dc1-9c56-388ca0739a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_document_chunks = split_list_of_document_chunks(list_of_document_chunks, 41000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca4f309-f021-427e-8e21-709ec5bfc95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split_chunk in split_document_chunks:\n",
    "    create_vectordb(split_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc058a1-e533-477f-b8ee-76e454115dd4",
   "metadata": {},
   "source": [
    "## Query our vector database\n",
    "- we use similarity search here\n",
    "- number of retrieved documents `k = 4` (default)\n",
    "- continued in *part 3*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdd3de27-461f-4ac1-9c18-93115d8d95a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load database from disk\n",
    "loaded_vectordb = Chroma(persist_directory=\"./chroma_db\", embedding_function=embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64f3dca9-260b-44bd-b828-3b4289fab3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Wer arbeitet an Blockchain-Technologie?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28062a92-67d1-4fae-90c9-a6860df1e14a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19bc78ff8b624dea9bf7aadb0b95bab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = loaded_vectordb.similarity_search(query, k=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78504f3-2a06-4ff1-8ab4-17603580525e",
   "metadata": {},
   "source": [
    "#### We retrieve semantically similar chunks, from different sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc626ca-82d3-4129-a8b7-f05163b5a53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content of a single retrieved chunk\n",
    "print(results[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6147be05-4989-417a-a83b-70a3424ffca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source of a single retrieved chunk\n",
    "print(results[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a74cb517-390f-43a9-9323-fc5ca745190b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# Number of different sources retrieved\n",
    "sources = set()\n",
    "for result in results:\n",
    "    sources.add(result.metadata[\"source\"])\n",
    "\n",
    "print(len(sources))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
